\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Transformers}{114}{chapter.7}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Motivation}{114}{section.7.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces Types of RNNs for modeling sequences}}{114}{figure.7.1}\protected@file@percent }
\newlabel{fig:enter-label}{{7.1}{114}{Types of RNNs for modeling sequences}{figure.7.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Attention Mechanism}{114}{section.7.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces Visual Attention}}{114}{figure.7.2}\protected@file@percent }
\newlabel{fig:enter-label}{{7.2}{114}{Visual Attention}{figure.7.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.3}{\ignorespaces Importance of words in a sentence}}{115}{figure.7.3}\protected@file@percent }
\newlabel{fig:enter-label}{{7.3}{115}{Importance of words in a sentence}{figure.7.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.4}{\ignorespaces RNN with attention vs RNN without attention}}{115}{figure.7.4}\protected@file@percent }
\newlabel{fig:enter-label}{{7.4}{115}{RNN with attention vs RNN without attention}{figure.7.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.5}{\ignorespaces Cross-Attention}}{116}{figure.7.5}\protected@file@percent }
\newlabel{fig:enter-label}{{7.5}{116}{Cross-Attention}{figure.7.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.6}{\ignorespaces Self-Attention (Inta-Attention)}}{116}{figure.7.6}\protected@file@percent }
\newlabel{fig:enter-label}{{7.6}{116}{Self-Attention (Inta-Attention)}{figure.7.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.3}Transformers}{117}{section.7.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.7}{\ignorespaces Scaled Dot-Product Attention}}{118}{figure.7.7}\protected@file@percent }
\newlabel{fig:enter-label}{{7.7}{118}{Scaled Dot-Product Attention}{figure.7.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.8}{\ignorespaces Multi-Head Attention}}{118}{figure.7.8}\protected@file@percent }
\newlabel{fig:enter-label}{{7.8}{118}{Multi-Head Attention}{figure.7.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.9}{\ignorespaces Transformer Encoder}}{119}{figure.7.9}\protected@file@percent }
\newlabel{fig:enter-label}{{7.9}{119}{Transformer Encoder}{figure.7.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.10}{\ignorespaces Positional Encoder}}{120}{figure.7.10}\protected@file@percent }
\newlabel{fig:enter-label}{{7.10}{120}{Positional Encoder}{figure.7.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.4}PyTorch Implementation}{120}{section.7.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.11}{\ignorespaces PyTorch Implementation of a Transformer Encoder}}{120}{figure.7.11}\protected@file@percent }
\newlabel{fig:enter-label}{{7.11}{120}{PyTorch Implementation of a Transformer Encoder}{figure.7.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.12}{\ignorespaces PyTorch Implementation of a Transformer Classifier}}{121}{figure.7.12}\protected@file@percent }
\newlabel{fig:enter-label}{{7.12}{121}{PyTorch Implementation of a Transformer Classifier}{figure.7.12}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.5}Language Modeling}{121}{section.7.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.13}{\ignorespaces Static Embeddings}}{121}{figure.7.13}\protected@file@percent }
\newlabel{fig:enter-label}{{7.13}{121}{Static Embeddings}{figure.7.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.14}{\ignorespaces Contextual Embeddings}}{122}{figure.7.14}\protected@file@percent }
\newlabel{fig:enter-label}{{7.14}{122}{Contextual Embeddings}{figure.7.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.15}{\ignorespaces Input Embeddings}}{123}{figure.7.15}\protected@file@percent }
\newlabel{fig:enter-label}{{7.15}{123}{Input Embeddings}{figure.7.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.16}{\ignorespaces Masked Word Prediction}}{123}{figure.7.16}\protected@file@percent }
\newlabel{fig:enter-label}{{7.16}{123}{Masked Word Prediction}{figure.7.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.17}{\ignorespaces Next Sentence Prediction}}{124}{figure.7.17}\protected@file@percent }
\newlabel{fig:enter-label}{{7.17}{124}{Next Sentence Prediction}{figure.7.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.18}{\ignorespaces Transfer Learning}}{124}{figure.7.18}\protected@file@percent }
\newlabel{fig:enter-label}{{7.18}{124}{Transfer Learning}{figure.7.18}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.6}Computer Vision}{125}{section.7.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.19}{\ignorespaces ViT}}{125}{figure.7.19}\protected@file@percent }
\newlabel{fig:enter-label}{{7.19}{125}{ViT}{figure.7.19}{}}
\@setckpt{chapters/transformers}{
\setcounter{page}{126}
\setcounter{equation}{0}
\setcounter{enumi}{3}
\setcounter{enumii}{1}
\setcounter{enumiii}{3}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{7}
\setcounter{section}{6}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{19}
\setcounter{table}{0}
\setcounter{tcbbreakpart}{1}
\setcounter{tcblayer}{0}
\setcounter{tcolorbox@number}{175}
\setcounter{tcbrastercolumn}{1}
\setcounter{tcbrasterrow}{1}
\setcounter{tcbrasternum}{1}
\setcounter{tcbraster}{0}
\setcounter{parentequation}{0}
\setcounter{lstnumber}{1}
\setcounter{tcblisting}{0}
\setcounter{section@level}{0}
\setcounter{Item}{100}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{60}
\setcounter{lstlisting}{0}
}
